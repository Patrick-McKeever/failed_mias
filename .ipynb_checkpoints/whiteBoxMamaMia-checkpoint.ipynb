{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7439d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson, nbinom, norm, multivariate_normal, truncnorm\n",
    "from numpy.linalg import cholesky, LinAlgError\n",
    "\n",
    "def nb_theta_mu_to_r_p(theta, mu):\n",
    "    r = theta\n",
    "    p = theta / (theta + mu)\n",
    "    return r, p\n",
    "\n",
    "def cdf_poisson(k, mu):\n",
    "    return poisson.cdf(k, mu)\n",
    "\n",
    "def cdf_nb_theta_mu(k, theta, mu):\n",
    "    r, p = nb_theta_mu_to_r_p(theta, mu)\n",
    "    return nbinom.cdf(k, r, p)\n",
    "\n",
    "def cdf_zip(k, mu, pi):\n",
    "    if k < 0:\n",
    "        return 0.0\n",
    "    if k == 0:\n",
    "        return pi + (1 - pi) * poisson.pmf(0, mu)\n",
    "    return pi + (1 - pi) * poisson.cdf(k, mu)\n",
    "\n",
    "def cdf_zinb_theta_mu(k, theta, mu, pi):\n",
    "    r, p = nb_theta_mu_to_r_p(theta, mu)\n",
    "    if k < 0:\n",
    "        return 0.0\n",
    "    if k == 0:\n",
    "        return pi + (1 - pi) * nbinom.pmf(0, r, p)\n",
    "    return pi + (1 - pi) * nbinom.cdf(k, r, p)\n",
    "\n",
    "# Compute P(a < Z < b) for Z ~ N(0, Sigma) w/ GHK.\n",
    "# Assume a and b have shape (p,), sigma is (p,p). Use M\n",
    "# draws.\n",
    "def ghk_estimate_rectangle_prob(a, b, Sigma, M=10000):\n",
    "    p = Sigma.shape[0]\n",
    "    try:\n",
    "        L = cholesky(Sigma)\n",
    "    except LinAlgError:\n",
    "        raise ValueError(\"Sigma is not positive definite\")\n",
    "    weights = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        z = np.zeros(p)\n",
    "        w = 1.0\n",
    "        for i in range(p):\n",
    "            mu_cond = np.dot(L[i, :i], z[:i])\n",
    "            sigma_cond = L[i, i]\n",
    "            # compute lower and upper truncation in u-space\n",
    "            a_u = (a[i] - mu_cond) / sigma_cond\n",
    "            b_u = (b[i] - mu_cond) / sigma_cond\n",
    "            # truncated normal draw for u_i\n",
    "            u_i = truncnorm.rvs(a_u, b_u, loc=0.0, scale=1.0)\n",
    "            # weight contribution for this step\n",
    "            w *= (norm.cdf(b_u) - norm.cdf(a_u))\n",
    "            \n",
    "            z[i] = mu_cond + sigma_cond * u_i\n",
    "        weights[m] = w\n",
    "    return weights.mean()\n",
    "\n",
    "# Estimate P(X=x) under Gaussian copula w/ discrete marginals.\n",
    "def gaussian_copula_point_probability_ghk(x, marg_params, Sigma,\n",
    "                                           eps=1e-12, M=5000):\n",
    "\n",
    "    x = np.asarray(x, dtype=int)\n",
    "    p = x.shape[0]\n",
    "    diag = np.sqrt(np.diag(Sigma))\n",
    "    R = Sigma / np.outer(diag, diag)\n",
    "    np.fill_diagonal(R, 1.0)\n",
    "\n",
    "    a = np.empty(p)\n",
    "    b = np.empty(p)\n",
    "    for i, xi in enumerate(x):\n",
    "        params = marg_params[i]\n",
    "        if params[0] == 0:\n",
    "            # non zero-inflated\n",
    "            if params[1] == np.inf:\n",
    "                mu = params[2]\n",
    "                a[i] = cdf_poisson(xi - 1, mu)\n",
    "                b[i] = cdf_poisson(xi, mu)\n",
    "            else:\n",
    "                theta = params[1]; mu = params[2]\n",
    "                a[i] = cdf_nb_theta_mu(xi - 1, theta, mu)\n",
    "                b[i] = cdf_nb_theta_mu(xi, theta, mu)\n",
    "        else:\n",
    "            # zero-inflated\n",
    "            pi = params[0]\n",
    "            if params[1] == np.inf:\n",
    "                mu = params[2]\n",
    "                a[i] = cdf_zip(xi - 1, mu, pi)\n",
    "                b[i] = cdf_zip(xi, mu, pi)\n",
    "            else:\n",
    "                theta = params[1]; mu = params[2]\n",
    "                a[i] = cdf_zinb_theta_mu(xi - 1, theta, mu, pi)\n",
    "                b[i] = cdf_zinb_theta_mu(xi, theta, mu, pi)\n",
    "        a[i] = np.clip(a[i], eps, 1 - eps)\n",
    "        b[i] = np.clip(b[i], eps, 1 - eps)\n",
    "\n",
    "    # z-space limits\n",
    "    z_lower = norm.ppf(a)\n",
    "    z_upper = norm.ppf(b)\n",
    "\n",
    "    # Estimate P(z_lower < Z < z_upper)\n",
    "    prob = ghk_estimate_rectangle_prob(z_lower, z_upper, R, M=M)\n",
    "    return prob\n",
    "\n",
    "def getPointScores(\n",
    "    targetAD, \n",
    "    auxMargParams, auxCovMat, auxCopulaGenes, \n",
    "    synthMargParams, synthCovMat, synthCopulaGenes\n",
    "):\n",
    "    scores = {}\n",
    "    for i, cell in enumerate(targetAD):\n",
    "        print(i)\n",
    "        try:\n",
    "            auxExpr = cell[:,auxCopulaGenes].X.toarray().flatten()\n",
    "            auxProb = gaussian_copula_point_probability_ghk(\n",
    "                auxExpr, auxMargParams, auxCovMat, M=100\n",
    "            )\n",
    "            synthExpr = cell[:,synthCopulaGenes].X.toarray().flatten()\n",
    "            synthProb = gaussian_copula_point_probability_ghk(\n",
    "                synthExpr, synthMargParams, synthCovMat, M=100\n",
    "            )\n",
    "            scores[i] = auxProb / synthProb\n",
    "        except:\n",
    "            scores[i] = None\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "961e0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import poisson, nbinom\n",
    "from numpy.linalg import cholesky, LinAlgError\n",
    "from math import sqrt\n",
    "from scipy.stats import norm as sp_norm\n",
    "\n",
    "def nb_theta_mu_to_r_p(theta, mu):\n",
    "    r = theta\n",
    "    p = theta / (theta + mu)\n",
    "    return r, p\n",
    "\n",
    "def cdf_poisson_vec(k_arr, mu_arr):\n",
    "    return poisson.cdf(k_arr, mu_arr)\n",
    "\n",
    "def cdf_nb_theta_mu_vec(k_arr, theta_arr, mu_arr):\n",
    "    r_arr, p_arr = theta_arr, theta_arr / (theta_arr + mu_arr)\n",
    "    return nbinom.cdf(k_arr, r_arr, p_arr)\n",
    "\n",
    "def cdf_zip_vec(k_arr, mu_arr, pi_arr):\n",
    "    k_arr = np.asarray(k_arr)\n",
    "    mu_arr = np.asarray(mu_arr)\n",
    "    pi_arr = np.asarray(pi_arr)\n",
    "    out = np.zeros_like(mu_arr, dtype=float)\n",
    "    mask_neg = k_arr < 0\n",
    "    out[mask_neg] = 0.0\n",
    "    mask_zero = (k_arr == 0)\n",
    "    if np.any(mask_zero):\n",
    "        out[mask_zero] = pi_arr[mask_zero] + (1 - pi_arr[mask_zero]) * poisson.pmf(0, mu_arr[mask_zero])\n",
    "    mask_pos = k_arr > 0\n",
    "    if np.any(mask_pos):\n",
    "        out[mask_pos] = pi_arr[mask_pos] + (1 - pi_arr[mask_pos]) * poisson.cdf(k_arr[mask_pos], mu_arr[mask_pos])\n",
    "    return out\n",
    "\n",
    "def cdf_zinb_theta_mu_vec(k_arr, theta_arr, mu_arr, pi_arr):\n",
    "    k_arr = np.asarray(k_arr)\n",
    "    theta_arr = np.asarray(theta_arr)\n",
    "    mu_arr = np.asarray(mu_arr)\n",
    "    pi_arr = np.asarray(pi_arr)\n",
    "    out = np.zeros_like(mu_arr, dtype=float)\n",
    "    mask_neg = k_arr < 0\n",
    "    out[mask_neg] = 0.0\n",
    "    mask_zero = (k_arr == 0)\n",
    "    if np.any(mask_zero):\n",
    "        r = theta_arr[mask_zero]\n",
    "        p = theta_arr[mask_zero] / (theta_arr[mask_zero] + mu_arr[mask_zero])\n",
    "        out[mask_zero] = pi_arr[mask_zero] + (1 - pi_arr[mask_zero]) * nbinom.pmf(0, r, p)\n",
    "    mask_pos = k_arr > 0\n",
    "    if np.any(mask_pos):\n",
    "        r = theta_arr[mask_pos]\n",
    "        p = theta_arr[mask_pos] / (theta_arr[mask_pos] + mu_arr[mask_pos])\n",
    "        out[mask_pos] = pi_arr[mask_pos] + (1 - pi_arr[mask_pos]) * nbinom.cdf(k_arr[mask_pos], r, p)\n",
    "    return out\n",
    "\n",
    "_torch_sqrt2 = np.sqrt(2.0)\n",
    "\n",
    "def torch_norm_cdf(x):\n",
    "    return 0.5 * (1.0 + torch.erf(x / _torch_sqrt2))\n",
    "\n",
    "def torch_norm_icdf(u):\n",
    "    return _torch_sqrt2 * torch.erfinv(2.0 * u - 1.0)\n",
    "\n",
    "# def ghk_estimate_rectangle_prob_torch_batch(z_lower, z_upper, R, M=10000, device=None, eps=1e-14):\n",
    "#     if device is None:\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     else:\n",
    "#         device = torch.device(device)\n",
    "\n",
    "#     z_lower = np.asarray(z_lower, dtype=float)\n",
    "#     z_upper = np.asarray(z_upper, dtype=float)\n",
    "#     R = np.asarray(R, dtype=float)\n",
    "\n",
    "#     B, p = z_lower.shape\n",
    "#     # Cholesky of R (correlation -> positive definite check)\n",
    "#     try:\n",
    "#         L = cholesky(R)\n",
    "#     except LinAlgError as e:\n",
    "#         raise ValueError(\"R is not positive definite\") from e\n",
    "#     # convert to torch on device\n",
    "#     L_t = torch.as_tensor(L, dtype=torch.float64, device=device)\n",
    "#     z_lower_t = torch.as_tensor(z_lower, dtype=torch.float64, device=device)\n",
    "#     z_upper_t = torch.as_tensor(z_upper, dtype=torch.float64, device=device)\n",
    "\n",
    "#     # We'll perform M draws per sample in parallel: tensors of shape (B, M)\n",
    "#     # Initialize log-weights for each (B,M)\n",
    "#     logw = torch.zeros((B, M), dtype=torch.float64, device=device)\n",
    "\n",
    "#     # z_i storage: we only need values up to current i\n",
    "#     # We'll keep z_prev as tensor of shape (i, B, M) initially empty\n",
    "#     z_prev = None  # will be a tensor (i, B, M)\n",
    "\n",
    "#     sqrt2 = _torch_sqrt2\n",
    "\n",
    "#     for i in range(p):\n",
    "#         # compute mu_cond = L[i, :i] @ z_prev (for each sample and each draw)\n",
    "#         if i == 0:\n",
    "#             mu_cond = torch.zeros((B, M), dtype=torch.float64, device=device)\n",
    "#         else:\n",
    "#             # z_prev: (i, B, M), L_row (i,)\n",
    "#             L_row = L_t[i, :i]  # vector length i\n",
    "#             # tensordot over axis 0 of z_prev (the gene axis) with L_row\n",
    "#             # result shape (B, M)\n",
    "#             mu_cond = torch.tensordot(L_row, z_prev, dims=([0], [0]))\n",
    "\n",
    "#         sigma_cond = float(L_t[i, i].item())  # scalar\n",
    "\n",
    "#         # compute truncation on standard normal (u-space):\n",
    "#         a_u = (z_lower_t[:, i:i+1] - mu_cond) / sigma_cond\n",
    "#         b_u = (z_upper_t[:, i:i+1] - mu_cond) / sigma_cond\n",
    "\n",
    "#         # compute Phi(a_u), Phi(b_u)\n",
    "#         Phi_a = torch_norm_cdf(a_u)\n",
    "#         Phi_b = torch_norm_cdf(b_u)\n",
    "\n",
    "#         # numerical safety: clamp differences away from 0\n",
    "#         diff = (Phi_b - Phi_a).clamp(min=eps)\n",
    "\n",
    "#         # sample uniform in [Phi_a, Phi_b] for each (B,M)\n",
    "#         U = torch.rand((B, M), dtype=torch.float64, device=device)\n",
    "#         U = Phi_a + U * (Phi_b - Phi_a)\n",
    "\n",
    "#         # invert to get standard normal truncated sample u_i\n",
    "#         u_i = torch_norm_icdf(U)\n",
    "\n",
    "#         # compute z_i = mu_cond + sigma_cond * u_i\n",
    "#         z_i = mu_cond + sigma_cond * u_i  # (B, M)\n",
    "\n",
    "#         # append z_i to z_prev\n",
    "#         if z_prev is None:\n",
    "#             z_prev = z_i.unsqueeze(0)  # shape (1, B, M)\n",
    "#         else:\n",
    "#             z_prev = torch.cat([z_prev, z_i.unsqueeze(0)], dim=0)  # shape (i+1, B, M)\n",
    "\n",
    "#         # update log-weights\n",
    "#         logw = logw + torch.log(diff)\n",
    "\n",
    "#     # weights: exp(logw) (shape B x M), average over M draws\n",
    "#     # To avoid overflow/underflow we can use log-sum-exp trick per row:\n",
    "#     # prob_b = mean_m exp(logw[b,m]) = exp(logsumexp(logw[b,:]) - log(M))\n",
    "#     # implement vectorized:\n",
    "#     max_logw, _ = logw.max(dim=1, keepdim=True)  # (B,1)\n",
    "#     exp_shift = torch.exp(logw - max_logw)  # (B,M)\n",
    "#     sum_exp = exp_shift.sum(dim=1)  # (B,)\n",
    "#     probs_t = (sum_exp * torch.exp(max_logw.squeeze(1))) / float(M)  # (B,)\n",
    "\n",
    "#     probs = probs_t.cpu().numpy()\n",
    "#     return probs\n",
    "\n",
    "def ghk_estimate_rectangle_prob_torch_batch(\n",
    "    z_lower, z_upper, R, M=10000, device=None, eps=1e-12, min_sigma=1e-12\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    z_lower = np.asarray(z_lower, dtype=float)\n",
    "    z_upper = np.asarray(z_upper, dtype=float)\n",
    "    R = np.asarray(R, dtype=float)\n",
    "\n",
    "    B, p = z_lower.shape\n",
    "    try:\n",
    "        L = cholesky(R)\n",
    "    except LinAlgError as e:\n",
    "        raise ValueError(\"R is not positive definite\") from e\n",
    "\n",
    "    L_t = torch.as_tensor(L, dtype=torch.float64, device=device)\n",
    "    z_lower_t = torch.as_tensor(z_lower, dtype=torch.float64, device=device)\n",
    "    z_upper_t = torch.as_tensor(z_upper, dtype=torch.float64, device=device)\n",
    "\n",
    "    logw = torch.zeros((B, M), dtype=torch.float64, device=device)\n",
    "    # z_prev = previuos (i, B, M)\n",
    "    z_prev = None\n",
    "\n",
    "    for i in range(p):\n",
    "        if i == 0:\n",
    "            mu_cond = torch.zeros((B, M), dtype=torch.float64, device=device)\n",
    "        else:\n",
    "            L_row = L_t[i, :i]\n",
    "            mu_cond = torch.tensordot(L_row, z_prev, dims=([0], [0]))\n",
    "\n",
    "        # conditional scale (coefficient of the new independent normal variate)\n",
    "        sigma_cond = float(L_t[i, i].item())\n",
    "        if sigma_cond <= 0.0:\n",
    "            raise ValueError(f\"Non-positive conditional scale L[{i},{i}]={sigma_cond}\")\n",
    "        # floor to avoid huge a_u/b_u from small sigma\n",
    "        if sigma_cond < min_sigma:\n",
    "            sigma_cond = min_sigma\n",
    "\n",
    "        a_u = (z_lower_t[:, i:i+1] - mu_cond) / sigma_cond\n",
    "        b_u = (z_upper_t[:, i:i+1] - mu_cond) / sigma_cond\n",
    "\n",
    "        Phi_a = torch_norm_cdf(a_u)\n",
    "        Phi_b = torch_norm_cdf(b_u)\n",
    "\n",
    "        diff = (Phi_b - Phi_a)\n",
    "        diff_clamped = diff.clamp(min=eps)\n",
    "\n",
    "        # sample uniformly in the clamped interval (prevents exact 0/1 and erfinv(Â±1))\n",
    "        U = Phi_a + torch.rand((B, M), dtype=torch.float64, device=device) * diff_clamped\n",
    "\n",
    "        U = U.clamp(min=eps, max=1.0 - eps)\n",
    "        u_i = torch_norm_icdf(U)\n",
    "        z_i = mu_cond + sigma_cond * u_i\n",
    "\n",
    "        # append z_i\n",
    "        if z_prev is None:\n",
    "            z_prev = z_i.unsqueeze(0)\n",
    "        else:\n",
    "            z_prev = torch.cat([z_prev, z_i.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # update log-weights using the clamped diff\n",
    "        logw = logw + torch.log(diff_clamped)\n",
    "\n",
    "    max_logw, _ = logw.max(dim=1, keepdim=True)\n",
    "    exp_shift = torch.exp(logw - max_logw)\n",
    "    sum_exp = exp_shift.sum(dim=1)\n",
    "    probs_t = (sum_exp * torch.exp(max_logw.squeeze(1))) / float(M)\n",
    "\n",
    "    probs = probs_t.cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Compute marginal A/B over cell batch.\n",
    "def build_ab_for_batch(x_batch, marg_params, eps=1e-12):\n",
    "    x_batch = np.asarray(x_batch, dtype=int)\n",
    "    B, p = x_batch.shape\n",
    "    a = np.empty((B, p), dtype=float)\n",
    "    b = np.empty((B, p), dtype=float)\n",
    "\n",
    "    # Vectorized over genes (loop over p is fine if p moderate)\n",
    "    for j in range(p):\n",
    "        params = marg_params[j]\n",
    "        xj = x_batch[:, j]\n",
    "        if params[0] == 0:\n",
    "            # not zero-inflated\n",
    "            if params[1] == np.inf:\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_poisson_vec(xj - 1, mu)\n",
    "                b[:, j] = cdf_poisson_vec(xj, mu)\n",
    "            else:\n",
    "                theta = np.full(B, params[1], dtype=float)\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_nb_theta_mu_vec(xj - 1, theta, mu)\n",
    "                b[:, j] = cdf_nb_theta_mu_vec(xj, theta, mu)\n",
    "        else:\n",
    "            pi = np.full(B, params[0], dtype=float)\n",
    "            if params[1] == np.inf:\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_zip_vec(xj - 1, mu, pi)\n",
    "                b[:, j] = cdf_zip_vec(xj, mu, pi)\n",
    "            else:\n",
    "                theta = np.full(B, params[1], dtype=float)\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_zinb_theta_mu_vec(xj - 1, theta, mu, pi)\n",
    "                b[:, j] = cdf_zinb_theta_mu_vec(xj, theta, mu, pi)\n",
    "\n",
    "        a[:, j] = np.clip(a[:, j], eps, 1 - eps)\n",
    "        b[:, j] = np.clip(b[:, j], eps, 1 - eps)\n",
    "    return a, b\n",
    "\n",
    "def gaussian_copula_point_probability_ghk_batch(\n",
    "    X_batch, marg_params, Sigma, M=5000, device=None\n",
    "):\n",
    "    # convert Sigma to correlation matrix R (like original)\n",
    "    diag = np.sqrt(np.diag(Sigma))\n",
    "    R = Sigma / np.outer(diag, diag)\n",
    "    np.fill_diagonal(R, 1.0)\n",
    "\n",
    "    # Build a and b (cdf bounds)\n",
    "    a_batch, b_batch = build_ab_for_batch(X_batch, marg_params)\n",
    "\n",
    "    z_lower = sp_norm.ppf(a_batch)\n",
    "    z_upper = sp_norm.ppf(b_batch)\n",
    "\n",
    "    probs = ghk_estimate_rectangle_prob_torch_batch(z_lower, z_upper, R, M=M, device=device)\n",
    "    return probs\n",
    "\n",
    "def extract_row(cell, genes):\n",
    "    arr = cell[:, genes].X\n",
    "    if hasattr(arr, \"toarray\"):\n",
    "        arr = arr.toarray().flatten()\n",
    "    else:\n",
    "        arr = np.asarray(arr).reshape(-1)\n",
    "    return arr.astype(int)\n",
    "\n",
    "def getPointScores_gpu(\n",
    "    targetAD,\n",
    "    auxMargParams, auxCovMat, auxCopulaGenes,\n",
    "    synthMargParams, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=32, M=2000, device=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    n_cells = len(targetAD)\n",
    "    scores = {}\n",
    "    idx = 0\n",
    "    noAuxNans = 0\n",
    "    noSynthNans = 0\n",
    "    \n",
    "    while idx < n_cells:\n",
    "        if idx % 10000 < batch_size: \n",
    "            print(f\"{idx} / {n_cells}\")\n",
    "        batch_cells = []\n",
    "        batch_indices = list(range(idx, min(idx + batch_size, n_cells)))\n",
    "        for i in batch_indices:\n",
    "            try:\n",
    "                r = extract_row(targetAD[i], auxCopulaGenes)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                r = None\n",
    "            batch_cells.append(r)\n",
    "\n",
    "        valid_mask = [row is not None for row in batch_cells]\n",
    "        if not any(valid_mask):\n",
    "            for ii, i_cell in enumerate(batch_indices):\n",
    "                scores[i_cell] = None\n",
    "            idx += batch_size\n",
    "            continue\n",
    "\n",
    "        valid_rows_idx = [ii for ii, ok in enumerate(valid_mask) if ok]\n",
    "        X_aux = np.vstack([batch_cells[ii] for ii in valid_rows_idx])\n",
    "        X_synth = np.vstack([\n",
    "            extract_row(targetAD[batch_indices[ii]], synthCopulaGenes)\n",
    "            for ii in valid_rows_idx\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            aux_probs = gaussian_copula_point_probability_ghk_batch(\n",
    "                X_aux, auxMargParams, auxCovMat, M=M, device=device\n",
    "            )\n",
    "        except Exception as e:\n",
    "            aux_probs = np.full((len(valid_rows_idx),), np.nan)\n",
    "            print(\"Error computing aux_probs for batch starting at\", idx, \":\", e)\n",
    "\n",
    "        try:\n",
    "            synth_probs = gaussian_copula_point_probability_ghk_batch(\n",
    "                X_synth, synthMargParams, synthCovMat, M=M, device=device\n",
    "            )\n",
    "        except Exception as e:\n",
    "            synth_probs = np.full((len(valid_rows_idx),), np.nan)\n",
    "            print(\"Error computing synth_probs for batch starting at\", idx, \":\", e)\n",
    "\n",
    "        vi = 0\n",
    "        minNonZero = np.min(aux_probs[aux_probs!=0])\n",
    "        minAuxProb = minNonZero\n",
    "        for ii, ok in enumerate(valid_mask):\n",
    "            global_idx = batch_indices[ii]\n",
    "            if not ok:\n",
    "                scores[global_idx] = None\n",
    "            else:\n",
    "                a_p = aux_probs[vi]\n",
    "                s_p = synth_probs[vi]\n",
    "                if np.isnan(a_p) or np.isnan(s_p):\n",
    "                    if np.isnan(a_p):\n",
    "                        noAuxNans += 1\n",
    "                    if np.isnan(s_p):\n",
    "                        noSynthNans += 1\n",
    "                        \n",
    "                    scores[global_idx] = None\n",
    "                elif a_p == 0:\n",
    "                    scores[global_idx] = float(s_p / minAuxProb)\n",
    "                else:\n",
    "                    scores[global_idx] = float(s_p / a_p)\n",
    "                    #scores[global_idx] = float(s_p)\n",
    "                vi += 1\n",
    "        idx += batch_size\n",
    "    #print(f\"% SYNTH NANS: {noSynthNans / n_cells}\")\n",
    "    #print(f\"% SYNTH NANS: {noAuxNans / n_cells}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a970ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def getIndScores(auxAD, trainAD, auxModelDir, trainModelDir, split=None):\n",
    "    cellTypes = trainAD.obs['cell_type'].value_counts().sort_values(ascending=True).index.tolist()\n",
    "    individualScores = {}\n",
    "    trainInds = set(trainAD.obs.individual.unique())\n",
    "    for ct in cellTypes:\n",
    "        nTrue = auxAD[(auxAD.obs.cell_type==ct)&(auxAD.obs.individual.isin(trainInds))].n_obs\n",
    "        nTotal = auxAD[(auxAD.obs.cell_type==ct)].n_obs\n",
    "        print(f\"PERCENT MEMBERS: {nTrue/nTotal}\")\n",
    "        if auxAD[auxAD.obs.cell_type==ct].n_obs > 20000:\n",
    "            continue\n",
    "        synthModel = rdata.read_rds(f\"{trainModelDir}/{ct}.rds\")[str(ct)]\n",
    "        #synthCopulaGenes = synthModel['gene_sel1']\n",
    "        #synthCopulaGenes = [x - 1 for x in synthModel['gene_sel1']]\n",
    "        synthCopulaGenes = synthModel['marginal_param1'].coords[\"dim_0\"].data\n",
    "        synthCovMat = synthModel['cov_mat']\n",
    "        synthCopulaMarginals = synthModel['marginal_param1']\n",
    "\n",
    "        auxModel = rdata.read_rds(f\"{auxModelDir}/{ct}.rds\")[str(ct)]\n",
    "        #auxCopulaGenes = auxModel['gene_sel1']\n",
    "        #auxCopulaGenes = [x - 1 for x in auxModel['gene_sel1']]\n",
    "        auxCopulaGenes = auxModel['marginal_param1'].coords[\"dim_0\"].data\n",
    "        \n",
    "        auxCovMat = auxModel['cov_mat']\n",
    "        auxCopulaMarginals = auxModel['marginal_param1']\n",
    "\n",
    "        auxCT = auxAD[auxAD.obs.cell_type==ct]\n",
    "        all_inds = auxCT.obs[\"individual\"].astype(str)\n",
    "        actualLabels = all_inds.isin(trainInds).tolist()\n",
    "        \n",
    "        scores = getPointScores_gpu(\n",
    "            auxCT,\n",
    "            auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "            synthCopulaMarginals, synthCovMat, synthCopulaGenes,\n",
    "            batch_size=500, M=2000, device=None\n",
    "        )\n",
    "        with open(f'scores/split{split}_ct{ct}.pickle', 'wb') as handle:\n",
    "            pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        nonUnderflowScores = [x for x in scores.values() if x is not None]\n",
    "        if len(nonUnderflowScores) == 0:\n",
    "            print(f\"Underflows affected all cell types in ct {ct}\")\n",
    "            continue\n",
    "        percentile_66 = np.percentile(nonUnderflowScores, 66)\n",
    "        #percentile_66 = np.percentile(nonUnderflowScores, 90)\n",
    "        min_score = np.min(nonUnderflowScores)\n",
    "        max_score = np.max(nonUnderflowScores)\n",
    "\n",
    "        predictedLabels = np.interp(\n",
    "            [score if score is not None else np.nan for score in scores.values()],\n",
    "            (min_score, max_score),\n",
    "            (0, 1)\n",
    "        )\n",
    "#         predictedLabels = [bool(random.getrandbits(1)) for i in range(len(scores))]\n",
    "#         for ind, score in scores.items():\n",
    "#             if score is not None:\n",
    "#                 predictedLabels[ind] = score > percentile_66\n",
    "                \n",
    "        auroc = roc_auc_score(actualLabels, predictedLabels)\n",
    "        print(f\"AUROC CT {ct}:\", auroc)\n",
    "#         tn, fp, fn, tp = confusion_matrix(actualLabels, predictedLabels).ravel()\n",
    "#         print(f\"CT {ct}: TP {tp / nTotal}, FP {fp / nTotal}\")\n",
    "        for ind, label in enumerate(predictedLabels):\n",
    "            individual = auxCT.obs.iloc[ind][\"individual\"]\n",
    "            if individual not in individualScores:\n",
    "                individualScores[individual] = 0\n",
    "            individualScores[individual] += label\n",
    "                \n",
    "    for individual, sumScore in individualScores.items():\n",
    "        nCells = auxAD[auxAD.obs.individual==individual].n_obs\n",
    "        individualScores[individual] /= nCells\n",
    "    \n",
    "    return individualScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24e55142",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3\n",
      "PERCENT MEMBERS: 0.3172645739910314\n",
      "0 / 1784\n",
      "AUROC CT 8: 0.8057625023934274\n",
      "PERCENT MEMBERS: 0.4180618975139523\n",
      "0 / 1971\n",
      "AUROC CT 2: 0.536271277541243\n",
      "PERCENT MEMBERS: 0.29753966429064155\n",
      "0 / 4349\n",
      "AUROC CT 6: 0.6516422263651702\n",
      "PERCENT MEMBERS: 0.2981670929241262\n",
      "0 / 4692\n",
      "AUROC CT 15: 0.6530034142212986\n",
      "PERCENT MEMBERS: 0.32225063938618925\n",
      "0 / 7038\n",
      "AUROC CT 10: 0.7042020232271805\n",
      "PERCENT MEMBERS: 0.342255350386809\n",
      "0 / 17709\n",
      "10000 / 17709\n",
      "AUROC CT 5: 0.4411277385273528\n",
      "PERCENT MEMBERS: 0.36346033373198927\n",
      "0 / 17559\n",
      "10000 / 17559\n",
      "AUROC CT 9: 0.6541106864622467\n",
      "PERCENT MEMBERS: 0.3333931696809227\n",
      "PERCENT MEMBERS: 0.3414830563071769\n",
      "PERCENT MEMBERS: 0.32885756676557865\n",
      "PERCENT MEMBERS: 0.3262453709234261\n",
      "PERCENT MEMBERS: 0.31024163707254077\n",
      "PERCENT MEMBERS: 0.293648705279264\n",
      "PERCENT MEMBERS: 0.3172738155590868\n",
      "Score 0.5738066736495586\n",
      "Split 1\n",
      "PERCENT MEMBERS: 0.3054932735426009\n",
      "0 / 1784\n",
      "AUROC CT 8: 0.8083272245299924\n",
      "PERCENT MEMBERS: 0.3333333333333333\n",
      "0 / 1971\n",
      "AUROC CT 2: 0.48854045764035126\n",
      "PERCENT MEMBERS: 0.33731892389054957\n",
      "0 / 4349\n",
      "AUROC CT 6: 0.5691505037732734\n",
      "PERCENT MEMBERS: 0.3523017902813299\n",
      "0 / 4692\n",
      "AUROC CT 15: 0.6391910208626831\n",
      "PERCENT MEMBERS: 0.39116226200625176\n",
      "0 / 7038\n",
      "AUROC CT 10: 0.7066097406838662\n",
      "PERCENT MEMBERS: 0.3353097295160653\n",
      "0 / 17709\n",
      "10000 / 17709\n",
      "AUROC CT 5: 0.4548929113426169\n",
      "PERCENT MEMBERS: 0.35879036391594055\n",
      "0 / 17559\n",
      "10000 / 17559\n",
      "AUROC CT 9: 0.6465980372668355\n",
      "PERCENT MEMBERS: 0.32540501727774535\n",
      "PERCENT MEMBERS: 0.35108233475067646\n",
      "PERCENT MEMBERS: 0.3152324431256182\n",
      "PERCENT MEMBERS: 0.32932146696929876\n",
      "PERCENT MEMBERS: 0.36072962972160827\n",
      "PERCENT MEMBERS: 0.3428841524622597\n",
      "PERCENT MEMBERS: 0.34859903463965136\n",
      "Score 0.5204405170634697\n",
      "Split 2\n",
      "PERCENT MEMBERS: 0.24860476915271434\n",
      "0 / 1971\n",
      "AUROC CT 2: 0.7675963565709876\n",
      "PERCENT MEMBERS: 0.3772421524663677\n",
      "0 / 1784\n",
      "AUROC CT 8: 0.8170530277396238\n",
      "PERCENT MEMBERS: 0.36514141181880894\n",
      "0 / 4349\n",
      "AUROC CT 6: 0.6327822440487649\n",
      "PERCENT MEMBERS: 0.3495311167945439\n",
      "0 / 4692\n",
      "AUROC CT 15: 0.6596056564268132\n",
      "PERCENT MEMBERS: 0.286587098607559\n",
      "0 / 7038\n",
      "AUROC CT 10: 0.6738636744019195\n",
      "PERCENT MEMBERS: 0.2777493023520702\n",
      "0 / 17559\n",
      "10000 / 17559\n",
      "AUROC CT 9: 0.6191653583694283\n",
      "PERCENT MEMBERS: 0.32243492009712577\n",
      "0 / 17709\n",
      "10000 / 17709\n",
      "AUROC CT 5: 0.5076293573209325\n",
      "PERCENT MEMBERS: 0.34120181304133196\n",
      "PERCENT MEMBERS: 0.30743460894214664\n",
      "PERCENT MEMBERS: 0.35590999010880314\n",
      "PERCENT MEMBERS: 0.3444331621072751\n",
      "PERCENT MEMBERS: 0.32902873320585097\n",
      "PERCENT MEMBERS: 0.3634671422584764\n",
      "PERCENT MEMBERS: 0.33412714980126185\n",
      "Score 0.6169677867206994\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import rdata\n",
    "\n",
    "\n",
    "def runMIA(auxAdPath, trainAdPath, auxModelDir, trainModelDir, split=None):\n",
    "    auxAD = sc.read_h5ad(auxAdPath)\n",
    "    trainAD = sc.read_h5ad(trainAdPath)\n",
    "    trainInds = set(trainAD.obs.individual.unique())\n",
    "    indScores = getIndScores(auxAD, trainAD, auxModelDir, trainModelDir, split=split)\n",
    "    percentile_66 = np.percentile(list(indScores.values()), 66)\n",
    "    predictedLabels = []\n",
    "    actualLabels = []\n",
    "    \n",
    "    for ind, score in indScores.items():\n",
    "        if score is not None:\n",
    "            predictedLabels.append(score > percentile_66)\n",
    "            actualLabels.append(ind in trainInds)\n",
    "    \n",
    "    return roc_auc_score(actualLabels, predictedLabels)\n",
    "\n",
    "# def runMIA2(auxAdPath, trainAdPath, auxModelDir, trainModelDir, split=None):\n",
    "#     auxAD = sc.read_h5ad(auxAdPath)\n",
    "#     trainAD = sc.read_h5ad(trainAdPath)\n",
    "#     trainInds = set(trainAD.obs.individual.unique())\n",
    "#     for ct in auxAD.obs.cell_type.unique():\n",
    "#         if auxAD[auxAD.obs.cell_type==ct].n_obs > 20000:\n",
    "#             continue\n",
    "#         with open(f\"scores/split{split}_ct{ct}.pickle\", \"rb\") as f:\n",
    "#             scores = pickle.load(f)\n",
    "            \n",
    "#         print(scores)\n",
    "# #     indScores = getIndScores(auxAD, trainAD, auxModelDir, trainModelDir, split=split)\n",
    "# #     percentile_66 = np.percentile(list(indScores.values()), 66)\n",
    "# #     predictedLabels = []\n",
    "# #     actualLabels = []\n",
    "    \n",
    "# #     for ind, score in indScores.items():\n",
    "# #         if score is not None:\n",
    "# #             predictedLabels.append(score > percentile_66)\n",
    "# #             actualLabels.append(ind in trainInds)\n",
    "    \n",
    "# #     return roc_auc_score(actualLabels, predictedLabels)\n",
    "\n",
    "for i in [3,1,2]:\n",
    "    print(f\"Split {i}\")\n",
    "    score = runMIA(\"train.h5ad\", f\"splits/{i}/train.h5ad\", \"models\", f\"splits/{i}/model\", split=str(i))\n",
    "    print(f\"Score {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a21cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "ad1 = sc.read_h5ad(\"splits/1/train.h5ad\")\n",
    "ad2 = sc.read_h5ad(\"splits/2/train.h5ad\")\n",
    "ad3 = sc.read_h5ad(\"splits/3/train.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e489a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 CT 0 density: 0.3819313199512026\n",
      "Split 2 CT 0 density: 0.3636809894173149\n",
      "Split 3 CT 0 density: 0.36867927061793365\n",
      "Split 1 CT 3 density: 0.411434081633677\n",
      "Split 2 CT 3 density: 0.40001329004354264\n",
      "Split 3 CT 3 density: 0.4011154692536468\n",
      "Split 1 CT 13 density: 0.3989216445716977\n",
      "Split 2 CT 13 density: 0.394272856997793\n",
      "Split 3 CT 13 density: 0.39271322519123103\n",
      "Split 1 CT 14 density: 0.42907974013721534\n",
      "Split 2 CT 14 density: 0.42166655073733483\n",
      "Split 3 CT 14 density: 0.4123314276335586\n",
      "Split 1 CT 4 density: 0.3727609770669855\n",
      "Split 2 CT 4 density: 0.3792973647738235\n",
      "Split 3 CT 4 density: 0.3779255782125085\n",
      "Split 1 CT 12 density: 0.3931845113390907\n",
      "Split 2 CT 12 density: 0.3922982953299932\n",
      "Split 3 CT 12 density: 0.390269994544849\n",
      "Split 1 CT 8 density: 0.46740015175553645\n",
      "Split 2 CT 8 density: 0.46753461166536847\n",
      "Split 3 CT 8 density: 0.45435563472826634\n",
      "Split 1 CT 5 density: 0.38541265697926175\n",
      "Split 2 CT 5 density: 0.38972019712458766\n",
      "Split 3 CT 5 density: 0.39405083406420705\n",
      "Split 1 CT 6 density: 0.43635883320006674\n",
      "Split 2 CT 6 density: 0.4189397132187906\n",
      "Split 3 CT 6 density: 0.41334600602841376\n",
      "Split 1 CT 1 density: 0.3840708905737806\n",
      "Split 2 CT 1 density: 0.373923781583044\n",
      "Split 3 CT 1 density: 0.36871965865949197\n",
      "Split 1 CT 10 density: 0.5447708312243584\n",
      "Split 2 CT 10 density: 0.5312403809326876\n",
      "Split 3 CT 10 density: 0.5343756454867561\n",
      "Split 1 CT 9 density: 0.5079921665635961\n",
      "Split 2 CT 9 density: 0.48797660544636146\n",
      "Split 3 CT 9 density: 0.48651890259536656\n",
      "Split 1 CT 2 density: 0.38979390349253373\n",
      "Split 2 CT 2 density: 0.3952611553095816\n",
      "Split 3 CT 2 density: 0.40095220313666907\n",
      "Split 1 CT 15 density: 0.42687264484434967\n",
      "Split 2 CT 15 density: 0.4420590994371482\n",
      "Split 3 CT 15 density: 0.41730378842029975\n"
     ]
    }
   ],
   "source": [
    "ads = {1: ad1, 2: ad2, 3: ad3}\n",
    "cts = ad1.obs.cell_type.unique()\n",
    "for ct in cts:\n",
    "    for split in [1,2,3]:\n",
    "        rModel = rdata.read_rds(f\"splits/{split}/model/{ct}.rds\")\n",
    "        copulaGenes = [x-1 for x in rModel[str(ct)]['gene_sel1']]\n",
    "        ad = ads[split]\n",
    "        expr = ad[ad.obs.cell_type==ct,copulaGenes].X != 0\n",
    "        #print(expr.shape)\n",
    "        density = (expr.sum(axis=1) / expr.shape[1]).mean()\n",
    "        density = expr.mean()\n",
    "        #print(density.shape)\n",
    "        print(f\"Split {split} CT {ct} density: {density}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cac99f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81827\n"
     ]
    }
   ],
   "source": [
    "print(ad1[ad1.obs.cell_type==0].n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6c257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2\n",
      "0 / 1784\n",
      "673 1111\n",
      "0 0\n",
      "No valid scores for cell type 8\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2532395/4030578113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Split {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunMIA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.h5ad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"splits/{i}/train.h5ad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"splits/{i}/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Score {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2532395/4030578113.py\u001b[0m in \u001b[0;36mrunMIA\u001b[0;34m(auxAdPath, trainAdPath, auxModelDir, trainModelDir)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mtrainInds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainAD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mindScores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetIndScoresNew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauxAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainModelDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mpercentile_66\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m66\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mpredictedLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mactualLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[1;32m   4281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_quantile_is_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4282\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percentiles must be in the range [0, 100]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4283\u001b[0;31m     return _quantile_unchecked(\n\u001b[0m\u001b[1;32m   4284\u001b[0m         a, q, axis, out, overwrite_input, method, keepdims)\n\u001b[1;32m   4285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[1;32m   4553\u001b[0m                         keepdims=False):\n\u001b[1;32m   4554\u001b[0m     \u001b[0;34m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4555\u001b[0;31m     return _ureduce(a,\n\u001b[0m\u001b[1;32m   4556\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_quantile_ureduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4557\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3821\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindex_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3823\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3825\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[1;32m   4720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4721\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4722\u001b[0;31m     result = _quantile(arr,\n\u001b[0m\u001b[1;32m   4723\u001b[0m                        \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4724\u001b[0m                        \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile\u001b[0;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[1;32m   4829\u001b[0m             axis=0)\n\u001b[1;32m   4830\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msupports_nans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4831\u001b[0;31m             \u001b[0mslices_having_nans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4832\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4833\u001b[0m             \u001b[0mslices_having_nans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# ---------- Batched getPointScores (synthetic probs only) ----------\n",
    "def getPointScores_gpuNew(\n",
    "    targetAD,\n",
    "    synthMargParams, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=32, M=2000, device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute synthetic probabilities only, batched.\n",
    "\n",
    "    targetAD: AnnData-like object or list of cell slices\n",
    "    synthMargParams, synthCovMat, synthCopulaGenes: parameters for synthetic model\n",
    "    batch_size: number of cells processed together\n",
    "    M: MC draws for GHK\n",
    "    device: cuda/cpu\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def extract_row(cell, genes):\n",
    "        arr = cell[:, genes].X\n",
    "        if hasattr(arr, \"toarray\"):\n",
    "            arr = arr.toarray().flatten()\n",
    "        else:\n",
    "            arr = np.asarray(arr).reshape(-1)\n",
    "        return arr.astype(int)\n",
    "\n",
    "    n_cells = len(targetAD)\n",
    "    scores = {}\n",
    "    idx = 0\n",
    "\n",
    "    while idx < n_cells:\n",
    "        if idx % 10000 < batch_size:\n",
    "            print(f\"{idx} / {n_cells}\")\n",
    "\n",
    "        batch_cells = []\n",
    "        batch_indices = list(range(idx, min(idx + batch_size, n_cells)))\n",
    "        for i in batch_indices:\n",
    "            try:\n",
    "                r = extract_row(targetAD[i], synthCopulaGenes)\n",
    "            except Exception:\n",
    "                r = None\n",
    "            batch_cells.append(r)\n",
    "\n",
    "        valid_mask = [row is not None for row in batch_cells]\n",
    "        if not any(valid_mask):\n",
    "            for ii, i_cell in enumerate(batch_indices):\n",
    "                scores[i_cell] = None\n",
    "            idx += batch_size\n",
    "            continue\n",
    "\n",
    "        valid_rows_idx = [ii for ii, ok in enumerate(valid_mask) if ok]\n",
    "        X_synth = np.vstack([batch_cells[ii] for ii in valid_rows_idx])\n",
    "\n",
    "        try:\n",
    "            synth_probs = gaussian_copula_point_probability_ghk_batch(\n",
    "                X_synth, synthMargParams, synthCovMat, M=M, device=device\n",
    "            )\n",
    "        except Exception as e:\n",
    "            synth_probs = np.full((len(valid_rows_idx),), np.nan)\n",
    "            print(\"Error computing synth_probs for batch starting at\", idx, \":\", e)\n",
    "\n",
    "        vi = 0\n",
    "        for ii, ok in enumerate(valid_mask):\n",
    "            global_idx = batch_indices[ii]\n",
    "            if not ok or np.isnan(synth_probs[vi]):\n",
    "                scores[global_idx] = None\n",
    "            else:\n",
    "                scores[global_idx] = float(synth_probs[vi])\n",
    "                vi += 1\n",
    "        idx += batch_size\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ---------- Get individual scores via statistical test ----------\n",
    "def getIndScoresNew(auxAD, trainAD, synthModelDir):\n",
    "    \"\"\"\n",
    "    Compare synthetic probabilities between train individuals and others.\n",
    "    Uses Mann-Whitney U test per cell type.\n",
    "    \"\"\"\n",
    "    import rdata\n",
    "    individualScores = {}\n",
    "    trainInds = set(trainAD.obs.individual.unique())\n",
    "    cellTypes = auxAD.obs.cell_type.unique()\n",
    "\n",
    "#     for ct in cellTypes:\n",
    "    for ct in [8]:\n",
    "        auxCT = auxAD[auxAD.obs.cell_type == ct]\n",
    "        if auxCT.n_obs == 0:\n",
    "            continue\n",
    "\n",
    "        synthModel = rdata.read_rds(f\"{synthModelDir}/{ct}.rds\")[str(ct)]\n",
    "        synthCopulaGenes = [x - 1 for x in synthModel['gene_sel1']]\n",
    "        synthCovMat = synthModel['cov_mat']\n",
    "        synthCopulaMarginals = synthModel['marginal_param1']\n",
    "\n",
    "        scores = getPointScores_gpuNew(\n",
    "            auxCT,\n",
    "            synthCopulaMarginals, synthCovMat, synthCopulaGenes,\n",
    "            batch_size=500, M=2000, device=None\n",
    "        )\n",
    "\n",
    "        all_scores = np.array([scores[i] if scores[i] is not None else np.nan for i in range(len(scores))])\n",
    "        labels = auxCT.obs['individual'].astype(str).isin(trainInds).to_numpy()\n",
    "\n",
    "        # Separate train vs other cells\n",
    "        train_scores = all_scores[labels]\n",
    "        other_scores = all_scores[~labels]\n",
    "\n",
    "        # Remove NaNs\n",
    "        print(len(train_scores), len(other_scores))\n",
    "        train_scores = train_scores[~np.isnan(train_scores)]\n",
    "        other_scores = other_scores[~np.isnan(other_scores)]\n",
    "        print(len(train_scores), len(other_scores))\n",
    "        if len(train_scores) == 0 or len(other_scores) == 0:\n",
    "            print(f\"No valid scores for cell type {ct}\")\n",
    "            continue\n",
    "\n",
    "        # Mann-Whitney U test (one-sided: train > others)\n",
    "        stat, pval = mannwhitneyu(train_scores, other_scores, alternative='greater')\n",
    "        print(f\"CT {ct}: Mann-Whitney U p-value = {pval:.3e}\")\n",
    "\n",
    "        # Aggregate per individual: mean synthetic probability\n",
    "        for ind in auxCT.obs['individual'].unique():\n",
    "            ind_mask = auxCT.obs['individual'] == ind\n",
    "            ind_scores = all_scores[ind_mask.to_numpy()]\n",
    "            ind_scores = ind_scores[~np.isnan(ind_scores)]\n",
    "            if len(ind_scores) == 0:\n",
    "                continue\n",
    "            individualScores[ind] = np.mean(ind_scores)\n",
    "\n",
    "    return individualScores\n",
    "\n",
    "def runMIA(auxAdPath, trainAdPath, auxModelDir, trainModelDir):\n",
    "    auxAD = sc.read_h5ad(auxAdPath)\n",
    "    trainAD = sc.read_h5ad(trainAdPath)\n",
    "    trainInds = set(trainAD.obs.individual.unique())\n",
    "    indScores = getIndScoresNew(auxAD, trainAD, trainModelDir)\n",
    "    percentile_66 = np.percentile(list(indScores.values()), 66)\n",
    "    predictedLabels = []\n",
    "    actualLabels = []\n",
    "    \n",
    "    for ind, score in indScores.items():\n",
    "        if score is not None:\n",
    "            predictedLabels.append(score > percentile_66)\n",
    "            actualLabels.append(ind in trainInds)\n",
    "    \n",
    "    return roc_auc_score(actualLabels, predictedLabels)\n",
    "\n",
    "for i in [2,3,1]:\n",
    "    print(f\"Split {i}\")\n",
    "    score = runMIA(\"train.h5ad\", f\"splits/{i}/train.h5ad\", \"models\", f\"splits/{i}/model\")\n",
    "    print(f\"Score {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41da23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f92b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresGPU = getPointScores_gpu(\n",
    "    adata_sample,\n",
    "    auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "    synthCopulaMarginals, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=32, M=2000, device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "107fcd9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rdata\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "synthModel = rdata.read_rds(\"splits/1/model/0.rds\")['0']\n",
    "synthCopulaGenes = synthModel['gene_sel1']\n",
    "synthCovMat = synthModel['cov_mat']\n",
    "synthCopulaMarginals = synthModel['marginal_param1']\n",
    "\n",
    "auxModel = rdata.read_rds(\"models/0.rds\")['0']\n",
    "auxCopulaGenes = auxModel['gene_sel1']\n",
    "auxCovMat = auxModel['cov_mat']\n",
    "auxCopulaMarginals = auxModel['marginal_param1']\n",
    "\n",
    "auxAD = sc.read_h5ad(\"train.h5ad\")\n",
    "auxCT0 = auxAD[auxAD.obs.cell_type==0]\n",
    "\n",
    "trainAD = sc.read_h5ad(\"splits/1/train.h5ad\")\n",
    "trainInds = set(trainAD.obs.individual.unique())\n",
    "all_inds = auxCT0.obs[\"individual\"].astype(str)\n",
    "actualLabels = all_inds.isin(trainInds).tolist()\n",
    "\n",
    "\n",
    "\n",
    "# actualLabels = []\n",
    "# for i, cell in enumerate(auxCT0):\n",
    "#     ind = cell.obs[\"individual\"].astype(str)[0]\n",
    "#     if ind in trainInds:\n",
    "#         actualLabels.append(True)\n",
    "#     else:\n",
    "#         actualLabels.append(False)\n",
    "\n",
    "# idx = np.random.choice(auxCT0.n_obs, size=1000, replace=False)\n",
    "# adata_sample = auxCT0[idx].copy()\n",
    "\n",
    "# scores = getPointScores(\n",
    "#     adata_sample, auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "#     synthCopulaMarginals, synthCovMat, synthCopulaGenes\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38583a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type\n",
       "0     234731\n",
       "3     102609\n",
       "14     80534\n",
       "4      66968\n",
       "13     40440\n",
       "1      31044\n",
       "12     22283\n",
       "5      17709\n",
       "9      17559\n",
       "10      7038\n",
       "15      4692\n",
       "6       4349\n",
       "2       1971\n",
       "8       1784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxAD.obs.cell_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac2acff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs Ã n_vars = 1 Ã 1118\n",
       "    obs: 'individual', 'cell_type', 'cell_label', 'barcode_col', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'\n",
       "    var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxCT0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4bbd97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 234731\n",
      "1000 / 234731\n",
      "2000 / 234731\n",
      "3000 / 234731\n",
      "4000 / 234731\n",
      "5000 / 234731\n",
      "6000 / 234731\n",
      "7000 / 234731\n",
      "8000 / 234731\n",
      "9000 / 234731\n",
      "10000 / 234731\n",
      "11000 / 234731\n",
      "12000 / 234731\n",
      "13000 / 234731\n",
      "14000 / 234731\n",
      "15000 / 234731\n",
      "16000 / 234731\n",
      "17000 / 234731\n",
      "18000 / 234731\n",
      "19000 / 234731\n",
      "20000 / 234731\n",
      "21000 / 234731\n",
      "22000 / 234731\n",
      "23000 / 234731\n",
      "24000 / 234731\n",
      "25000 / 234731\n",
      "26000 / 234731\n",
      "27000 / 234731\n",
      "28000 / 234731\n",
      "29000 / 234731\n",
      "30000 / 234731\n",
      "31000 / 234731\n",
      "32000 / 234731\n",
      "33000 / 234731\n",
      "34000 / 234731\n",
      "35000 / 234731\n",
      "36000 / 234731\n",
      "37000 / 234731\n",
      "38000 / 234731\n",
      "39000 / 234731\n",
      "40000 / 234731\n",
      "41000 / 234731\n",
      "42000 / 234731\n",
      "43000 / 234731\n",
      "44000 / 234731\n",
      "45000 / 234731\n",
      "46000 / 234731\n",
      "47000 / 234731\n",
      "48000 / 234731\n",
      "49000 / 234731\n",
      "50000 / 234731\n",
      "51000 / 234731\n",
      "52000 / 234731\n",
      "53000 / 234731\n",
      "54000 / 234731\n",
      "55000 / 234731\n",
      "56000 / 234731\n",
      "57000 / 234731\n",
      "58000 / 234731\n",
      "59000 / 234731\n",
      "60000 / 234731\n",
      "61000 / 234731\n",
      "62000 / 234731\n",
      "63000 / 234731\n",
      "64000 / 234731\n",
      "65000 / 234731\n",
      "66000 / 234731\n",
      "67000 / 234731\n",
      "68000 / 234731\n",
      "69000 / 234731\n",
      "70000 / 234731\n",
      "71000 / 234731\n",
      "72000 / 234731\n",
      "73000 / 234731\n",
      "74000 / 234731\n",
      "75000 / 234731\n",
      "76000 / 234731\n",
      "77000 / 234731\n",
      "78000 / 234731\n",
      "79000 / 234731\n",
      "80000 / 234731\n",
      "81000 / 234731\n",
      "82000 / 234731\n",
      "83000 / 234731\n",
      "84000 / 234731\n",
      "85000 / 234731\n",
      "86000 / 234731\n",
      "87000 / 234731\n",
      "88000 / 234731\n",
      "89000 / 234731\n",
      "90000 / 234731\n",
      "91000 / 234731\n",
      "92000 / 234731\n",
      "93000 / 234731\n",
      "94000 / 234731\n",
      "95000 / 234731\n",
      "96000 / 234731\n",
      "97000 / 234731\n",
      "98000 / 234731\n",
      "99000 / 234731\n",
      "100000 / 234731\n",
      "101000 / 234731\n",
      "102000 / 234731\n",
      "103000 / 234731\n",
      "104000 / 234731\n",
      "105000 / 234731\n",
      "106000 / 234731\n",
      "107000 / 234731\n",
      "108000 / 234731\n",
      "109000 / 234731\n",
      "110000 / 234731\n",
      "111000 / 234731\n",
      "112000 / 234731\n",
      "113000 / 234731\n",
      "114000 / 234731\n",
      "115000 / 234731\n",
      "116000 / 234731\n",
      "117000 / 234731\n",
      "118000 / 234731\n",
      "119000 / 234731\n",
      "120000 / 234731\n",
      "121000 / 234731\n",
      "122000 / 234731\n",
      "123000 / 234731\n",
      "124000 / 234731\n",
      "125000 / 234731\n",
      "126000 / 234731\n",
      "127000 / 234731\n",
      "128000 / 234731\n",
      "129000 / 234731\n",
      "130000 / 234731\n",
      "131000 / 234731\n",
      "132000 / 234731\n",
      "133000 / 234731\n",
      "134000 / 234731\n",
      "135000 / 234731\n",
      "136000 / 234731\n",
      "137000 / 234731\n",
      "138000 / 234731\n",
      "139000 / 234731\n",
      "140000 / 234731\n",
      "141000 / 234731\n",
      "142000 / 234731\n",
      "143000 / 234731\n",
      "144000 / 234731\n",
      "145000 / 234731\n",
      "146000 / 234731\n",
      "147000 / 234731\n",
      "148000 / 234731\n",
      "149000 / 234731\n",
      "150000 / 234731\n",
      "151000 / 234731\n",
      "152000 / 234731\n",
      "153000 / 234731\n",
      "154000 / 234731\n",
      "155000 / 234731\n",
      "156000 / 234731\n",
      "157000 / 234731\n",
      "158000 / 234731\n",
      "159000 / 234731\n",
      "160000 / 234731\n",
      "161000 / 234731\n",
      "162000 / 234731\n",
      "163000 / 234731\n",
      "164000 / 234731\n",
      "165000 / 234731\n",
      "166000 / 234731\n",
      "167000 / 234731\n",
      "168000 / 234731\n",
      "169000 / 234731\n",
      "170000 / 234731\n",
      "171000 / 234731\n",
      "172000 / 234731\n",
      "173000 / 234731\n",
      "174000 / 234731\n",
      "175000 / 234731\n",
      "176000 / 234731\n",
      "177000 / 234731\n",
      "178000 / 234731\n",
      "179000 / 234731\n",
      "180000 / 234731\n",
      "181000 / 234731\n",
      "182000 / 234731\n",
      "183000 / 234731\n",
      "184000 / 234731\n",
      "185000 / 234731\n",
      "186000 / 234731\n",
      "187000 / 234731\n",
      "188000 / 234731\n",
      "189000 / 234731\n",
      "190000 / 234731\n",
      "191000 / 234731\n",
      "192000 / 234731\n",
      "193000 / 234731\n",
      "194000 / 234731\n",
      "195000 / 234731\n",
      "196000 / 234731\n",
      "197000 / 234731\n",
      "198000 / 234731\n",
      "199000 / 234731\n",
      "200000 / 234731\n",
      "201000 / 234731\n",
      "202000 / 234731\n",
      "203000 / 234731\n",
      "204000 / 234731\n",
      "205000 / 234731\n",
      "206000 / 234731\n",
      "207000 / 234731\n",
      "208000 / 234731\n",
      "209000 / 234731\n",
      "210000 / 234731\n",
      "211000 / 234731\n",
      "212000 / 234731\n",
      "213000 / 234731\n",
      "214000 / 234731\n",
      "215000 / 234731\n",
      "216000 / 234731\n",
      "217000 / 234731\n",
      "218000 / 234731\n",
      "219000 / 234731\n",
      "220000 / 234731\n",
      "221000 / 234731\n",
      "222000 / 234731\n",
      "223000 / 234731\n",
      "224000 / 234731\n",
      "225000 / 234731\n",
      "226000 / 234731\n",
      "227000 / 234731\n",
      "228000 / 234731\n",
      "229000 / 234731\n",
      "230000 / 234731\n",
      "231000 / 234731\n",
      "232000 / 234731\n",
      "233000 / 234731\n",
      "234000 / 234731\n"
     ]
    }
   ],
   "source": [
    "scoresGPU = getPointScores_gpu(\n",
    "    auxCT0,\n",
    "    auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "    synthCopulaMarginals, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=1000, M=2000, device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "401630d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oldScores = scores\n",
    "scores = scoresGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a2198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f9a60f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "percentile_66 = np.percentile([x for x in scores.values() if x is not None], 66)\n",
    "predictedLabels = [bool(random.getrandbits(1)) for i in range(len(scores))]\n",
    "for ind, score in scores.items():\n",
    "    if score is not None:\n",
    "        predictedLabels[ind] = score > percentile_66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4e78a331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.5399298219241364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(actualLabels, predictedLabels)\n",
    "print(\"AUROC:\", auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2499df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lysander/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "trainAD = sc.read_h5ad(\"splits/1/train.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "808552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct0 = trainAD[trainAD.obs.cell_type==0, copulaGenes]\n",
    "cell = ct0[0].X.toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17b2c340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.530491408619522e-17"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_copula_point_probability_ghk(cell, copulaMarginals, covMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7edbdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    vals.append(gaussian_copula_point_probability_ghk(cell, copulaMarginals, covMat, M=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ba72d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.94688859631545e-17"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_copula_point_probability_ghk(cell, copulaMarginals, covMat, M=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "36473157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(trainAD):\n",
    "    print(i)\n",
    "    print(c.X.toarray().flatten())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
