{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7439d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson, nbinom, norm, multivariate_normal, truncnorm\n",
    "from numpy.linalg import cholesky, LinAlgError\n",
    "\n",
    "def nb_theta_mu_to_r_p(theta, mu):\n",
    "    r = theta\n",
    "    p = theta / (theta + mu)\n",
    "    return r, p\n",
    "\n",
    "def cdf_poisson(k, mu):\n",
    "    return poisson.cdf(k, mu)\n",
    "\n",
    "def cdf_nb_theta_mu(k, theta, mu):\n",
    "    r, p = nb_theta_mu_to_r_p(theta, mu)\n",
    "    return nbinom.cdf(k, r, p)\n",
    "\n",
    "def cdf_zip(k, mu, pi):\n",
    "    if k < 0:\n",
    "        return 0.0\n",
    "    if k == 0:\n",
    "        return pi + (1 - pi) * poisson.pmf(0, mu)\n",
    "    return pi + (1 - pi) * poisson.cdf(k, mu)\n",
    "\n",
    "def cdf_zinb_theta_mu(k, theta, mu, pi):\n",
    "    r, p = nb_theta_mu_to_r_p(theta, mu)\n",
    "    if k < 0:\n",
    "        return 0.0\n",
    "    if k == 0:\n",
    "        return pi + (1 - pi) * nbinom.pmf(0, r, p)\n",
    "    return pi + (1 - pi) * nbinom.cdf(k, r, p)\n",
    "\n",
    "def ghk_estimate_rectangle_prob(a, b, Sigma, M=10000):\n",
    "    \"\"\"\n",
    "    Estimate P(a < Z < b) for Z ~ N(0, Sigma) using GHK.\n",
    "    a, b: arrays of length p (lower, upper)\n",
    "    Sigma: correlation matrix (p x p)\n",
    "    M: number of simulation draws\n",
    "    Returns: estimate of probability\n",
    "    \"\"\"\n",
    "    p = Sigma.shape[0]\n",
    "    # Cholesky: Sigma = L L^T\n",
    "    try:\n",
    "        L = cholesky(Sigma)\n",
    "    except LinAlgError:\n",
    "        raise ValueError(\"Sigma is not positive definite\")\n",
    "    # We simulate M samples\n",
    "    weights = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        # sequential draw\n",
    "        z = np.zeros(p)\n",
    "        w = 1.0\n",
    "        for i in range(p):\n",
    "            # compute conditional mean and variance of z_i given previous z[0:i]\n",
    "            # because z = L * u, u ~ N(0,I). So we can simulate u sequentially.\n",
    "            mu_cond = np.dot(L[i, :i], z[:i])\n",
    "            sigma_cond = L[i, i]\n",
    "            # compute lower and upper truncation in u-space\n",
    "            a_u = (a[i] - mu_cond) / sigma_cond\n",
    "            b_u = (b[i] - mu_cond) / sigma_cond\n",
    "            # truncated normal draw for u_i\n",
    "            u_i = truncnorm.rvs(a_u, b_u, loc=0.0, scale=1.0)\n",
    "            # weight contribution for this step\n",
    "            w *= (norm.cdf(b_u) - norm.cdf(a_u))\n",
    "            # set z_i = mu_cond + sigma_cond * u_i\n",
    "            z[i] = mu_cond + sigma_cond * u_i\n",
    "        weights[m] = w\n",
    "    return weights.mean()\n",
    "\n",
    "def gaussian_copula_point_probability_ghk(x, marg_params, Sigma,\n",
    "                                           eps=1e-12, M=5000):\n",
    "    \"\"\"\n",
    "    Estimate Pr(X == x) under Gaussian copula + discrete marginals,\n",
    "    using GHK simulation with M draws.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=int)\n",
    "    p = x.shape[0]\n",
    "    # verify/correct Sigma to correlation matrix (like earlier)\n",
    "    diag = np.sqrt(np.diag(Sigma))\n",
    "    R = Sigma / np.outer(diag, diag)\n",
    "    np.fill_diagonal(R, 1.0)\n",
    "\n",
    "    # compute a_i, b_i\n",
    "    a = np.empty(p)\n",
    "    b = np.empty(p)\n",
    "    for i, xi in enumerate(x):\n",
    "        params = marg_params[i]\n",
    "        if params[0] == 0:\n",
    "            # non zero-inflated\n",
    "            if params[1] == np.inf:\n",
    "                mu = params[2]\n",
    "                a[i] = cdf_poisson(xi - 1, mu)\n",
    "                b[i] = cdf_poisson(xi, mu)\n",
    "            else:\n",
    "                theta = params[1]; mu = params[2]\n",
    "                a[i] = cdf_nb_theta_mu(xi - 1, theta, mu)\n",
    "                b[i] = cdf_nb_theta_mu(xi, theta, mu)\n",
    "        else:\n",
    "            # zero-inflated\n",
    "            pi = params[0]\n",
    "            if params[1] == np.inf:\n",
    "                mu = params[2]\n",
    "                a[i] = cdf_zip(xi - 1, mu, pi)\n",
    "                b[i] = cdf_zip(xi, mu, pi)\n",
    "            else:\n",
    "                theta = params[1]; mu = params[2]\n",
    "                a[i] = cdf_zinb_theta_mu(xi - 1, theta, mu, pi)\n",
    "                b[i] = cdf_zinb_theta_mu(xi, theta, mu, pi)\n",
    "        a[i] = np.clip(a[i], eps, 1 - eps)\n",
    "        b[i] = np.clip(b[i], eps, 1 - eps)\n",
    "\n",
    "    # convert to z-space limits\n",
    "    z_lower = norm.ppf(a)\n",
    "    z_upper = norm.ppf(b)\n",
    "\n",
    "    # Estimate via GHK: P(z_lower < Z < z_upper)\n",
    "    prob = ghk_estimate_rectangle_prob(z_lower, z_upper, R, M=M)\n",
    "    return prob\n",
    "\n",
    "def getPointScores(\n",
    "    targetAD, \n",
    "    auxMargParams, auxCovMat, auxCopulaGenes, \n",
    "    synthMargParams, synthCovMat, synthCopulaGenes\n",
    "):\n",
    "    scores = {}\n",
    "    for i, cell in enumerate(targetAD):\n",
    "        print(i)\n",
    "        try:\n",
    "            auxExpr = cell[:,auxCopulaGenes].X.toarray().flatten()\n",
    "            auxProb = gaussian_copula_point_probability_ghk(\n",
    "                auxExpr, auxMargParams, auxCovMat, M=100\n",
    "            )\n",
    "            synthExpr = cell[:,synthCopulaGenes].X.toarray().flatten()\n",
    "            synthProb = gaussian_copula_point_probability_ghk(\n",
    "                synthExpr, synthMargParams, synthCovMat, M=100\n",
    "            )\n",
    "            scores[i] = auxProb / synthProb\n",
    "        except:\n",
    "            scores[i] = None\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74a838c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import poisson, nbinom\n",
    "from numpy.linalg import cholesky, LinAlgError\n",
    "from math import sqrt\n",
    "\n",
    "# ---------- Marginal CDF helpers (vectorized via scipy) ----------\n",
    "def nb_theta_mu_to_r_p(theta, mu):\n",
    "    r = theta\n",
    "    p = theta / (theta + mu)\n",
    "    return r, p\n",
    "\n",
    "def cdf_poisson_vec(k_arr, mu_arr):\n",
    "    # k_arr and mu_arr are numpy arrays (same shape) or broadcastable\n",
    "    # For k < 0 returns 0 automatically via scipy\n",
    "    return poisson.cdf(k_arr, mu_arr)\n",
    "\n",
    "def cdf_nb_theta_mu_vec(k_arr, theta_arr, mu_arr):\n",
    "    r_arr, p_arr = theta_arr, theta_arr / (theta_arr + mu_arr)\n",
    "    # scipy.nbinom.cdf accepts arrays for k, n (r), p\n",
    "    return nbinom.cdf(k_arr, r_arr, p_arr)\n",
    "\n",
    "def cdf_zip_vec(k_arr, mu_arr, pi_arr):\n",
    "    # pi in [0,1]\n",
    "    # vectorized: handle k<0 and k==0 properly\n",
    "    k_arr = np.asarray(k_arr)\n",
    "    mu_arr = np.asarray(mu_arr)\n",
    "    pi_arr = np.asarray(pi_arr)\n",
    "    out = np.zeros_like(mu_arr, dtype=float)\n",
    "    mask_neg = k_arr < 0\n",
    "    out[mask_neg] = 0.0\n",
    "    mask_zero = (k_arr == 0)\n",
    "    if np.any(mask_zero):\n",
    "        out[mask_zero] = pi_arr[mask_zero] + (1 - pi_arr[mask_zero]) * poisson.pmf(0, mu_arr[mask_zero])\n",
    "    mask_pos = k_arr > 0\n",
    "    if np.any(mask_pos):\n",
    "        out[mask_pos] = pi_arr[mask_pos] + (1 - pi_arr[mask_pos]) * poisson.cdf(k_arr[mask_pos], mu_arr[mask_pos])\n",
    "    return out\n",
    "\n",
    "def cdf_zinb_theta_mu_vec(k_arr, theta_arr, mu_arr, pi_arr):\n",
    "    k_arr = np.asarray(k_arr)\n",
    "    theta_arr = np.asarray(theta_arr)\n",
    "    mu_arr = np.asarray(mu_arr)\n",
    "    pi_arr = np.asarray(pi_arr)\n",
    "    out = np.zeros_like(mu_arr, dtype=float)\n",
    "    mask_neg = k_arr < 0\n",
    "    out[mask_neg] = 0.0\n",
    "    mask_zero = (k_arr == 0)\n",
    "    if np.any(mask_zero):\n",
    "        r = theta_arr[mask_zero]\n",
    "        p = theta_arr[mask_zero] / (theta_arr[mask_zero] + mu_arr[mask_zero])\n",
    "        out[mask_zero] = pi_arr[mask_zero] + (1 - pi_arr[mask_zero]) * nbinom.pmf(0, r, p)\n",
    "    mask_pos = k_arr > 0\n",
    "    if np.any(mask_pos):\n",
    "        r = theta_arr[mask_pos]\n",
    "        p = theta_arr[mask_pos] / (theta_arr[mask_pos] + mu_arr[mask_pos])\n",
    "        out[mask_pos] = pi_arr[mask_pos] + (1 - pi_arr[mask_pos]) * nbinom.cdf(k_arr[mask_pos], r, p)\n",
    "    return out\n",
    "\n",
    "# ---------- Torch Gaussian helpers (cdf/icdf) ----------\n",
    "_torch_sqrt2 = np.sqrt(2.0)\n",
    "\n",
    "def torch_norm_cdf(x):\n",
    "    # x: torch tensor (double)\n",
    "    # Φ(x) = 0.5 * (1 + erf(x / sqrt(2)))\n",
    "    return 0.5 * (1.0 + torch.erf(x / _torch_sqrt2))\n",
    "\n",
    "def torch_norm_icdf(u):\n",
    "    # inverse cdf using erfinv: Φ^{-1}(u) = sqrt(2) * erfinv(2u - 1)\n",
    "    # u in (0,1)\n",
    "    return _torch_sqrt2 * torch.erfinv(2.0 * u - 1.0)\n",
    "\n",
    "# ---------- GHK on GPU, batched over B samples and M draws ----------\n",
    "def ghk_estimate_rectangle_prob_torch_batch(z_lower, z_upper, R, M=10000, device=None, eps=1e-14):\n",
    "    \"\"\"\n",
    "    Vectorized GHK estimator on GPU using PyTorch.\n",
    "\n",
    "    Inputs:\n",
    "      z_lower: numpy array shape (B, p)\n",
    "      z_upper: numpy array shape (B, p)\n",
    "      R: correlation matrix (p, p) numpy array\n",
    "      M: number of Monte Carlo draws\n",
    "      device: 'cuda' or 'cpu' (if None, auto select)\n",
    "    Returns:\n",
    "      probs: numpy array shape (B,) estimated probabilities for each sample\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    z_lower = np.asarray(z_lower, dtype=float)\n",
    "    z_upper = np.asarray(z_upper, dtype=float)\n",
    "    R = np.asarray(R, dtype=float)\n",
    "\n",
    "    B, p = z_lower.shape\n",
    "    # Cholesky of R (correlation -> positive definite check)\n",
    "    try:\n",
    "        L = cholesky(R)\n",
    "    except LinAlgError as e:\n",
    "        raise ValueError(\"R is not positive definite\") from e\n",
    "    # convert to torch on device\n",
    "    L_t = torch.as_tensor(L, dtype=torch.float64, device=device)  # p x p\n",
    "    z_lower_t = torch.as_tensor(z_lower, dtype=torch.float64, device=device)  # B x p\n",
    "    z_upper_t = torch.as_tensor(z_upper, dtype=torch.float64, device=device)  # B x p\n",
    "\n",
    "    # We'll perform M draws per sample in parallel: tensors of shape (B, M)\n",
    "    # Initialize log-weights for each (B,M)\n",
    "    logw = torch.zeros((B, M), dtype=torch.float64, device=device)\n",
    "\n",
    "    # z_i storage: we only need values up to current i\n",
    "    # We'll keep z_prev as tensor of shape (i, B, M) initially empty\n",
    "    z_prev = None  # will be a tensor (i, B, M)\n",
    "\n",
    "    sqrt2 = _torch_sqrt2\n",
    "\n",
    "    for i in range(p):\n",
    "        # compute mu_cond = L[i, :i] @ z_prev (for each sample and each draw)\n",
    "        if i == 0:\n",
    "            mu_cond = torch.zeros((B, M), dtype=torch.float64, device=device)\n",
    "        else:\n",
    "            # z_prev: (i, B, M), L_row (i,)\n",
    "            L_row = L_t[i, :i]  # vector length i\n",
    "            # tensordot over axis 0 of z_prev (the gene axis) with L_row\n",
    "            # result shape (B, M)\n",
    "            mu_cond = torch.tensordot(L_row, z_prev, dims=([0], [0]))  # (B, M)\n",
    "\n",
    "        sigma_cond = float(L_t[i, i].item())  # scalar\n",
    "\n",
    "        # compute truncation on standard normal (u-space):\n",
    "        a_u = (z_lower_t[:, i:i+1] - mu_cond) / sigma_cond  # (B, M) broadcasting\n",
    "        b_u = (z_upper_t[:, i:i+1] - mu_cond) / sigma_cond\n",
    "\n",
    "        # compute Phi(a_u), Phi(b_u)\n",
    "        Phi_a = torch_norm_cdf(a_u)\n",
    "        Phi_b = torch_norm_cdf(b_u)\n",
    "\n",
    "        # numerical safety: clamp differences away from 0\n",
    "        diff = (Phi_b - Phi_a).clamp(min=eps)\n",
    "\n",
    "        # sample uniform in [Phi_a, Phi_b] for each (B,M)\n",
    "        U = torch.rand((B, M), dtype=torch.float64, device=device)\n",
    "        U = Phi_a + U * (Phi_b - Phi_a)\n",
    "\n",
    "        # invert to get standard normal truncated sample u_i\n",
    "        u_i = torch_norm_icdf(U)\n",
    "\n",
    "        # compute z_i = mu_cond + sigma_cond * u_i\n",
    "        z_i = mu_cond + sigma_cond * u_i  # (B, M)\n",
    "\n",
    "        # append z_i to z_prev\n",
    "        if z_prev is None:\n",
    "            z_prev = z_i.unsqueeze(0)  # shape (1, B, M)\n",
    "        else:\n",
    "            z_prev = torch.cat([z_prev, z_i.unsqueeze(0)], dim=0)  # shape (i+1, B, M)\n",
    "\n",
    "        # update log-weights\n",
    "        logw = logw + torch.log(diff)\n",
    "\n",
    "    # weights: exp(logw) (shape B x M), average over M draws\n",
    "    # To avoid overflow/underflow we can use log-sum-exp trick per row:\n",
    "    # prob_b = mean_m exp(logw[b,m]) = exp(logsumexp(logw[b,:]) - log(M))\n",
    "    # implement vectorized:\n",
    "    max_logw, _ = logw.max(dim=1, keepdim=True)  # (B,1)\n",
    "    exp_shift = torch.exp(logw - max_logw)  # (B,M)\n",
    "    sum_exp = exp_shift.sum(dim=1)  # (B,)\n",
    "    probs_t = (sum_exp * torch.exp(max_logw.squeeze(1))) / float(M)  # (B,)\n",
    "\n",
    "    probs = probs_t.cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# ---------- Build marginal a/b arrays for a batch of cells ----------\n",
    "def build_ab_for_batch(x_batch, marg_params, eps=1e-12):\n",
    "    \"\"\"\n",
    "    x_batch: numpy array shape (B,p) integer counts\n",
    "    marg_params: list length p where each element is (pi, theta, mu)\n",
    "      - pi==0 means not zero-inflated, theta==np.inf means Poisson\n",
    "      - same convention you used\n",
    "    Returns:\n",
    "      a_batch, b_batch: numpy arrays shape (B, p)\n",
    "    \"\"\"\n",
    "    x_batch = np.asarray(x_batch, dtype=int)\n",
    "    B, p = x_batch.shape\n",
    "    a = np.empty((B, p), dtype=float)\n",
    "    b = np.empty((B, p), dtype=float)\n",
    "\n",
    "    # Vectorized over genes (loop over p is fine if p moderate)\n",
    "    for j in range(p):\n",
    "        params = marg_params[j]\n",
    "        xj = x_batch[:, j]\n",
    "        if params[0] == 0:\n",
    "            # not zero-inflated\n",
    "            if params[1] == np.inf:\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_poisson_vec(xj - 1, mu)\n",
    "                b[:, j] = cdf_poisson_vec(xj, mu)\n",
    "            else:\n",
    "                theta = np.full(B, params[1], dtype=float)\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_nb_theta_mu_vec(xj - 1, theta, mu)\n",
    "                b[:, j] = cdf_nb_theta_mu_vec(xj, theta, mu)\n",
    "        else:\n",
    "            pi = np.full(B, params[0], dtype=float)\n",
    "            if params[1] == np.inf:\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_zip_vec(xj - 1, mu, pi)\n",
    "                b[:, j] = cdf_zip_vec(xj, mu, pi)\n",
    "            else:\n",
    "                theta = np.full(B, params[1], dtype=float)\n",
    "                mu = np.full(B, params[2], dtype=float)\n",
    "                a[:, j] = cdf_zinb_theta_mu_vec(xj - 1, theta, mu, pi)\n",
    "                b[:, j] = cdf_zinb_theta_mu_vec(xj, theta, mu, pi)\n",
    "        # clip\n",
    "        a[:, j] = np.clip(a[:, j], eps, 1 - eps)\n",
    "        b[:, j] = np.clip(b[:, j], eps, 1 - eps)\n",
    "    return a, b\n",
    "\n",
    "# ---------- High-level vectorized probability function ----------\n",
    "def gaussian_copula_point_probability_ghk_batch(\n",
    "    X_batch, marg_params, Sigma, M=5000, device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    X_batch: numpy array shape (B, p) with integer counts (each row is a cell)\n",
    "    marg_params: list length p (per-gene) as before\n",
    "    Sigma: covariance (p,p) - will be converted to correlation R\n",
    "    Returns:\n",
    "      probs: numpy array shape (B,)\n",
    "    \"\"\"\n",
    "    # convert Sigma to correlation matrix R (like original)\n",
    "    diag = np.sqrt(np.diag(Sigma))\n",
    "    R = Sigma / np.outer(diag, diag)\n",
    "    np.fill_diagonal(R, 1.0)\n",
    "\n",
    "    # Build a and b (cdf bounds)\n",
    "    a_batch, b_batch = build_ab_for_batch(X_batch, marg_params)\n",
    "\n",
    "    # z-limits\n",
    "    # use the normal inverse cdf on CPU (numpy) then push to torch inside ghk function\n",
    "    from scipy.stats import norm as sp_norm\n",
    "    z_lower = sp_norm.ppf(a_batch)\n",
    "    z_upper = sp_norm.ppf(b_batch)\n",
    "\n",
    "    # call GHK sampler (torch)\n",
    "    probs = ghk_estimate_rectangle_prob_torch_batch(z_lower, z_upper, R, M=M, device=device)\n",
    "    return probs\n",
    "\n",
    "# ---------- Batched getPointScores (works with your AnnData-like cells) ----------\n",
    "def getPointScores_gpu(\n",
    "    targetAD,\n",
    "    auxMargParams, auxCovMat, auxCopulaGenes,\n",
    "    synthMargParams, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=32, M=2000, device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    targetAD: iterable of cell slices (like your original code where each\n",
    "      'cell' is subsetting an AnnData row: cell[:, genes].X)\n",
    "      It's assumed each cell row produces a (1 x p) sparse matrix accessible by:\n",
    "          arr = cell[:, gene_indices].X.toarray().flatten()\n",
    "      If targetAD is a dense numpy array (n_cells x n_genes) you can adapt easily.\n",
    "    batch_size: number of cells processed together\n",
    "    M: MC draws for GHK\n",
    "    device: cuda/cpu (default auto)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # helper to extract a dense 1D array from the cell selection\n",
    "    def extract_row(cell, genes):\n",
    "        # keep original approach for compatibility\n",
    "        arr = cell[:, genes].X\n",
    "        # If it's sparse or dense, ensure numpy 1-D\n",
    "        if hasattr(arr, \"toarray\"):\n",
    "            arr = arr.toarray().flatten()\n",
    "        else:\n",
    "            arr = np.asarray(arr).reshape(-1)\n",
    "        return arr.astype(int)\n",
    "\n",
    "    # gather all cells into a list for indexing\n",
    "    n_cells = len(targetAD)\n",
    "    scores = {}\n",
    "    idx = 0\n",
    "    while idx < n_cells:\n",
    "        print(f\"{idx} / {n_cells}\")\n",
    "        # build batch\n",
    "        batch_cells = []\n",
    "        batch_indices = list(range(idx, min(idx + batch_size, n_cells)))\n",
    "        for i in batch_indices:\n",
    "            try:\n",
    "                r = extract_row(targetAD[i], auxCopulaGenes)\n",
    "            except Exception:\n",
    "                r = None\n",
    "            batch_cells.append(r)\n",
    "        # split into aux and synth arrays (some may be None)\n",
    "        # For those None, we will set result to None and skip heavy work\n",
    "        valid_mask = [row is not None for row in batch_cells]\n",
    "        if not any(valid_mask):\n",
    "            for ii, i_cell in enumerate(batch_indices):\n",
    "                scores[i_cell] = None\n",
    "            idx += batch_size\n",
    "            continue\n",
    "\n",
    "        # build X_aux_batch and X_synth_batch arrays for valid rows\n",
    "        valid_rows_idx = [ii for ii, ok in enumerate(valid_mask) if ok]\n",
    "        X_aux = np.vstack([batch_cells[ii] for ii in valid_rows_idx])  # shape (Bval, p_aux)\n",
    "        # For synth we need to extract synth genes per the same indices in targetAD\n",
    "        X_synth = np.vstack([\n",
    "            extract_row(targetAD[batch_indices[ii]], synthCopulaGenes)\n",
    "            for ii in valid_rows_idx\n",
    "        ])\n",
    "\n",
    "        # compute probs in batch (aux and synth)\n",
    "        try:\n",
    "            aux_probs = gaussian_copula_point_probability_ghk_batch(\n",
    "                X_aux, auxMargParams, auxCovMat, M=M, device=device\n",
    "            )\n",
    "        except Exception as e:\n",
    "            aux_probs = np.full((len(valid_rows_idx),), np.nan)\n",
    "            print(\"Error computing aux_probs for batch starting at\", idx, \":\", e)\n",
    "\n",
    "        try:\n",
    "            synth_probs = gaussian_copula_point_probability_ghk_batch(\n",
    "                X_synth, synthMargParams, synthCovMat, M=M, device=device\n",
    "            )\n",
    "        except Exception as e:\n",
    "            synth_probs = np.full((len(valid_rows_idx),), np.nan)\n",
    "            print(\"Error computing synth_probs for batch starting at\", idx, \":\", e)\n",
    "\n",
    "        # assign back to scores\n",
    "        vi = 0\n",
    "        for ii, ok in enumerate(valid_mask):\n",
    "            global_idx = batch_indices[ii]\n",
    "            if not ok:\n",
    "                scores[global_idx] = None\n",
    "            else:\n",
    "                a_p = aux_probs[vi]\n",
    "                s_p = synth_probs[vi]\n",
    "                # guard against division by zero/nan\n",
    "                if (s_p == 0) or np.isnan(a_p) or np.isnan(s_p):\n",
    "                    scores[global_idx] = None\n",
    "                else:\n",
    "                    scores[global_idx] = float(a_p / s_p)\n",
    "                vi += 1\n",
    "        idx += batch_size\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49e9ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresGPU = getPointScores_gpu(\n",
    "    adata_sample,\n",
    "    auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "    synthCopulaMarginals, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=32, M=2000, device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "107fcd9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rdata\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "synthModel = rdata.read_rds(\"splits/1/model/0.rds\")['0']\n",
    "synthCopulaGenes = synthModel['gene_sel1']\n",
    "synthCovMat = synthModel['cov_mat']\n",
    "synthCopulaMarginals = synthModel['marginal_param1']\n",
    "\n",
    "auxModel = rdata.read_rds(\"models/0.rds\")['0']\n",
    "auxCopulaGenes = auxModel['gene_sel1']\n",
    "auxCovMat = auxModel['cov_mat']\n",
    "auxCopulaMarginals = auxModel['marginal_param1']\n",
    "\n",
    "auxAD = sc.read_h5ad(\"train.h5ad\")\n",
    "auxCT0 = auxAD[auxAD.obs.cell_type==0]\n",
    "\n",
    "trainAD = sc.read_h5ad(\"splits/1/train.h5ad\")\n",
    "trainInds = set(trainAD.obs.individual.unique())\n",
    "all_inds = auxCT0.obs[\"individual\"].astype(str)\n",
    "actualLabels = all_inds.isin(trainInds).tolist()\n",
    "\n",
    "# actualLabels = []\n",
    "# for i, cell in enumerate(auxCT0):\n",
    "#     ind = cell.obs[\"individual\"].astype(str)[0]\n",
    "#     if ind in trainInds:\n",
    "#         actualLabels.append(True)\n",
    "#     else:\n",
    "#         actualLabels.append(False)\n",
    "\n",
    "# idx = np.random.choice(auxCT0.n_obs, size=1000, replace=False)\n",
    "# adata_sample = auxCT0[idx].copy()\n",
    "\n",
    "# scores = getPointScores(\n",
    "#     adata_sample, auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "#     synthCopulaMarginals, synthCovMat, synthCopulaGenes\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0e2a27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 234731\n",
      "1000 / 234731\n",
      "2000 / 234731\n",
      "3000 / 234731\n",
      "4000 / 234731\n",
      "5000 / 234731\n",
      "6000 / 234731\n",
      "7000 / 234731\n",
      "8000 / 234731\n",
      "9000 / 234731\n",
      "10000 / 234731\n",
      "11000 / 234731\n",
      "12000 / 234731\n",
      "13000 / 234731\n",
      "14000 / 234731\n",
      "15000 / 234731\n",
      "16000 / 234731\n",
      "17000 / 234731\n",
      "18000 / 234731\n",
      "19000 / 234731\n",
      "20000 / 234731\n",
      "21000 / 234731\n",
      "22000 / 234731\n",
      "23000 / 234731\n",
      "24000 / 234731\n",
      "25000 / 234731\n",
      "26000 / 234731\n",
      "27000 / 234731\n",
      "28000 / 234731\n",
      "29000 / 234731\n",
      "30000 / 234731\n",
      "31000 / 234731\n",
      "32000 / 234731\n",
      "33000 / 234731\n",
      "34000 / 234731\n",
      "35000 / 234731\n",
      "36000 / 234731\n",
      "37000 / 234731\n",
      "38000 / 234731\n",
      "39000 / 234731\n",
      "40000 / 234731\n",
      "41000 / 234731\n",
      "42000 / 234731\n",
      "43000 / 234731\n",
      "44000 / 234731\n",
      "45000 / 234731\n",
      "46000 / 234731\n",
      "47000 / 234731\n",
      "48000 / 234731\n",
      "49000 / 234731\n",
      "50000 / 234731\n",
      "51000 / 234731\n",
      "52000 / 234731\n",
      "53000 / 234731\n",
      "54000 / 234731\n",
      "55000 / 234731\n",
      "56000 / 234731\n",
      "57000 / 234731\n",
      "58000 / 234731\n",
      "59000 / 234731\n",
      "60000 / 234731\n",
      "61000 / 234731\n",
      "62000 / 234731\n",
      "63000 / 234731\n",
      "64000 / 234731\n",
      "65000 / 234731\n",
      "66000 / 234731\n",
      "67000 / 234731\n",
      "68000 / 234731\n",
      "69000 / 234731\n",
      "70000 / 234731\n",
      "71000 / 234731\n",
      "72000 / 234731\n",
      "73000 / 234731\n",
      "74000 / 234731\n",
      "75000 / 234731\n",
      "76000 / 234731\n",
      "77000 / 234731\n",
      "78000 / 234731\n",
      "79000 / 234731\n",
      "80000 / 234731\n",
      "81000 / 234731\n",
      "82000 / 234731\n",
      "83000 / 234731\n",
      "84000 / 234731\n",
      "85000 / 234731\n",
      "86000 / 234731\n",
      "87000 / 234731\n",
      "88000 / 234731\n",
      "89000 / 234731\n",
      "90000 / 234731\n",
      "91000 / 234731\n",
      "92000 / 234731\n",
      "93000 / 234731\n",
      "94000 / 234731\n",
      "95000 / 234731\n",
      "96000 / 234731\n",
      "97000 / 234731\n",
      "98000 / 234731\n",
      "99000 / 234731\n",
      "100000 / 234731\n",
      "101000 / 234731\n",
      "102000 / 234731\n",
      "103000 / 234731\n",
      "104000 / 234731\n",
      "105000 / 234731\n",
      "106000 / 234731\n",
      "107000 / 234731\n",
      "108000 / 234731\n",
      "109000 / 234731\n",
      "110000 / 234731\n",
      "111000 / 234731\n",
      "112000 / 234731\n",
      "113000 / 234731\n",
      "114000 / 234731\n",
      "115000 / 234731\n",
      "116000 / 234731\n",
      "117000 / 234731\n",
      "118000 / 234731\n",
      "119000 / 234731\n",
      "120000 / 234731\n",
      "121000 / 234731\n",
      "122000 / 234731\n",
      "123000 / 234731\n",
      "124000 / 234731\n",
      "125000 / 234731\n",
      "126000 / 234731\n",
      "127000 / 234731\n",
      "128000 / 234731\n",
      "129000 / 234731\n",
      "130000 / 234731\n",
      "131000 / 234731\n",
      "132000 / 234731\n",
      "133000 / 234731\n",
      "134000 / 234731\n",
      "135000 / 234731\n",
      "136000 / 234731\n",
      "137000 / 234731\n",
      "138000 / 234731\n",
      "139000 / 234731\n",
      "140000 / 234731\n",
      "141000 / 234731\n",
      "142000 / 234731\n",
      "143000 / 234731\n",
      "144000 / 234731\n",
      "145000 / 234731\n",
      "146000 / 234731\n",
      "147000 / 234731\n",
      "148000 / 234731\n",
      "149000 / 234731\n",
      "150000 / 234731\n",
      "151000 / 234731\n",
      "152000 / 234731\n",
      "153000 / 234731\n",
      "154000 / 234731\n",
      "155000 / 234731\n",
      "156000 / 234731\n",
      "157000 / 234731\n",
      "158000 / 234731\n",
      "159000 / 234731\n",
      "160000 / 234731\n",
      "161000 / 234731\n",
      "162000 / 234731\n",
      "163000 / 234731\n",
      "164000 / 234731\n",
      "165000 / 234731\n",
      "166000 / 234731\n",
      "167000 / 234731\n",
      "168000 / 234731\n",
      "169000 / 234731\n",
      "170000 / 234731\n",
      "171000 / 234731\n",
      "172000 / 234731\n",
      "173000 / 234731\n",
      "174000 / 234731\n",
      "175000 / 234731\n",
      "176000 / 234731\n",
      "177000 / 234731\n",
      "178000 / 234731\n",
      "179000 / 234731\n",
      "180000 / 234731\n",
      "181000 / 234731\n",
      "182000 / 234731\n",
      "183000 / 234731\n",
      "184000 / 234731\n",
      "185000 / 234731\n",
      "186000 / 234731\n",
      "187000 / 234731\n",
      "188000 / 234731\n",
      "189000 / 234731\n",
      "190000 / 234731\n",
      "191000 / 234731\n",
      "192000 / 234731\n",
      "193000 / 234731\n",
      "194000 / 234731\n",
      "195000 / 234731\n",
      "196000 / 234731\n",
      "197000 / 234731\n",
      "198000 / 234731\n",
      "199000 / 234731\n",
      "200000 / 234731\n",
      "201000 / 234731\n",
      "202000 / 234731\n",
      "203000 / 234731\n",
      "204000 / 234731\n",
      "205000 / 234731\n",
      "206000 / 234731\n",
      "207000 / 234731\n",
      "208000 / 234731\n",
      "209000 / 234731\n",
      "210000 / 234731\n",
      "211000 / 234731\n",
      "212000 / 234731\n",
      "213000 / 234731\n",
      "214000 / 234731\n",
      "215000 / 234731\n",
      "216000 / 234731\n",
      "217000 / 234731\n",
      "218000 / 234731\n",
      "219000 / 234731\n",
      "220000 / 234731\n",
      "221000 / 234731\n",
      "222000 / 234731\n",
      "223000 / 234731\n",
      "224000 / 234731\n",
      "225000 / 234731\n",
      "226000 / 234731\n",
      "227000 / 234731\n",
      "228000 / 234731\n",
      "229000 / 234731\n",
      "230000 / 234731\n",
      "231000 / 234731\n",
      "232000 / 234731\n",
      "233000 / 234731\n",
      "234000 / 234731\n"
     ]
    }
   ],
   "source": [
    "scoresGPU = getPointScores_gpu(\n",
    "    auxCT0,\n",
    "    auxCopulaMarginals, auxCovMat, auxCopulaGenes, \n",
    "    synthCopulaMarginals, synthCovMat, synthCopulaGenes,\n",
    "    batch_size=1000, M=2000, device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9a7436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oldScores = scores\n",
    "scores = scoresGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0990eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "percentile_66 = np.percentile([x for x in scores.values() if x is not None], 66)\n",
    "predictedLabels = [bool(random.getrandbits(1)) for i in range(len(scores))]\n",
    "for ind, score in scores.items():\n",
    "    if score is not None:\n",
    "        predictedLabels[ind] = score > percentile_66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d50409fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.5399298219241364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(actualLabels, predictedLabels)\n",
    "print(\"AUROC:\", auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2499df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lysander/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "trainAD = sc.read_h5ad(\"splits/1/train.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "808552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct0 = trainAD[trainAD.obs.cell_type==0, copulaGenes]\n",
    "cell = ct0[0].X.toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17b2c340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.530491408619522e-17"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_copula_point_probability_ghk(cell, copulaMarginals, covMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7edbdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    vals.append(gaussian_copula_point_probability_ghk(cell, copulaMarginals, covMat, M=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ba72d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.94688859631545e-17"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_copula_point_probability_ghk(cell, copulaMarginals, covMat, M=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "36473157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(trainAD):\n",
    "    print(i)\n",
    "    print(c.X.toarray().flatten())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scdesign2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
